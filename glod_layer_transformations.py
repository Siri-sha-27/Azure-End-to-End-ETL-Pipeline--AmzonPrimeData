# -*- coding: utf-8 -*-
"""Glod_layer_Transformations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Anfbm_CXkxNympMprNg_-dZXP5tVHTJI
"""

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")

dbutils.fs.ls('abfss://silver@adlsprimework.dfs.core.windows.net/')

from pyspark.sql.functions import *
from pyspark.sql.types import*

df_gold = spark.read.format("parquet")\
    .option("header", "true")\
    .option("inferSchema", "true")\
    .load("abfss://silver@adlsprimework.dfs.core.windows.net/amazon_prime_titles_silver.csv")
df_gold.display()

df_gold = df_gold.withColumn("date_added",to_date(df_gold["date_added"],"MM/dd/yyyy"))
df_gold = df_gold.withColumn("year_added",year(df_gold["date_added"]))
df_gold.display()

df_gold = df_gold.withColumn("category_1",split(df_gold["listed_in"],", ")[0])
df_gold = df_gold.withColumn("category_2",split(df_gold["listed_in"],", ")[1])

df_gold.display()

df_gold = df_gold.withColumn("category_2",when(df_gold["category_2"].isNull(),"Unkown").otherwise(df_gold["category_2"]))
df_gold.display()

# Extract duration value and unit from duration field
# Movies: "110 min" -> duration_value: 110, duration_unit: "min"
# TV Shows: "5 Seasons" -> duration_value: 5, duration_unit: "Seasons"

df_gold = df_gold.withColumn("duration_value",
                             regexp_extract(col("duration"), r"(\d+)", 1).cast("integer"))
df_gold = df_gold.withColumn("duration_unit",
                             regexp_extract(col("duration"), r"[a-zA-Z]+", 0))

df_gold.select("type", "duration", "duration_value", "duration_unit").display()

# Convert release_year from string to integer for analytics
df_gold = df_gold.withColumn("release_year", col("release_year").cast("integer"))

df_gold.select("content_title", "release_year").display()

# Calculate how old the content is (current year - release year)
from datetime import datetime
current_year = datetime.now().year

df_gold = df_gold.withColumn("content_age_years",
                             lit(current_year) - col("release_year"))

df_gold.select("content_title", "release_year", "content_age_years").display()

# Fix typo: "Unkown" -> "Unknown" to be consistent with other fields
df_gold = df_gold.withColumn("category_2",
                             when(col("category_2") == "Unkown", "Unknown")
                             .otherwise(col("category_2")))

df_gold.select("category_1", "category_2").display()

# Drop listed_in column since we've split it into category_1 and category_2
df_gold = df_gold.drop("listed_in")

print(f"Total columns after dropping listed_in: {len(df_gold.columns)}")
df_gold.display()

# Add flags for records with missing critical data
df_gold = df_gold.withColumn("has_director",
                             when((col("director") == "Unknown") | col("director").isNull(), 0)
                             .otherwise(1))

df_gold = df_gold.withColumn("has_cast",
                             when((col("cast") == "Unknown") | col("cast").isNull(), 0)
                             .otherwise(1))

df_gold = df_gold.withColumn("has_country",
                             when((col("country") == "Unknown") | col("country").isNull(), 0)
                             .otherwise(1))

# Create overall data quality score (0-3)
df_gold = df_gold.withColumn("data_quality_score",
                             col("has_director") + col("has_cast") + col("has_country"))

df_gold.select("content_title", "has_director", "has_cast", "has_country", "data_quality_score").display()

# Display final schema
print("=== Final Gold Layer Schema ===")
df_gold.printSchema()

print(f"\nTotal Records: {df_gold.count()}")
print(f"Total Columns: {len(df_gold.columns)}")

# Show sample of transformed data
print("\n=== Sample Transformed Data ===")
df_gold.display()

# The OAuth configuration from Cell 1 applies to all containers in the storage account
# Let's verify we can access the gold container
print("Checking gold container access...")
try:
    files = dbutils.fs.ls('abfss://gold@adlsprimework.dfs.core.windows.net/')
    print(f"✓ Successfully accessed gold container")
    print(f"  Found {len(files)} items")
except Exception as e:
    print(f"✗ Error accessing gold container: {e}")

df_gold.write.format("delta")\
    .mode("append")\
        .option("path","abfss://gold@adlsprimework.dfs.core.windows.net/amazon_prime_titles_primelast.csv")\
            .save()

# Commented out IPython magic to ensure Python compatibility.
# %sql
create database gold_layer;

# Create managed table without specifying external path
# Unity Catalog will manage the storage location automatically
try:
    df_gold.write.format("delta") \
        .mode("overwrite") \
        .saveAsTable("gold_layer.prime_gold")
    print("✓ Table gold_layer.prime_gold created successfully")
except Exception as e:
    print(f"Error with gold_layer database: {e}")
    print("\nFallback: Creating table in default database...")
    # Use default/hive_metastore database instead
    df_gold.write.format("delta") \
        .mode("overwrite") \
        .saveAsTable("default.prime_gold")
    print("✓ Table default.prime_gold created successfully")

# Commented out IPython magic to ensure Python compatibility.
# %sql
SELECT * FROM gold_layer.prime_gold
